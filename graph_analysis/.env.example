# Cognee 0.3.6 Configuration for OpenRouter + FastEmbed
#
# ============================================================================
# SETUP INSTRUCTIONS
# ============================================================================
#
# To create your .env file:
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Get your OpenRouter API key:
#    - Visit: https://openrouter.ai/keys
#    - Sign up or log in
#    - Create a new API key
#
# 3. Edit .env and replace YOUR-OPENROUTER-API-KEY-HERE with your actual key:
#    LLM_API_KEY=sk-or-v1-your-actual-key-here
#    OPENAI_API_KEY=sk-or-v1-your-actual-key-here
#
# 4. Save the file
#
# That's it! All other settings are pre-configured and ready to use.
#
# ============================================================================
# LLM Configuration (OpenRouter)
# ============================================================================
# Using OpenRouter directly (no proxy needed)
# Using Claude Sonnet 4.5 via OpenRouter
# IMPORTANT: Use "openai/" prefix to force OpenAI-compatible API format
LLM_PROVIDER=openai
LLM_MODEL=openai/anthropic/claude-sonnet-4.5
LLM_ENDPOINT=https://openrouter.ai/api/v1
LLM_API_KEY=YOUR-OPENROUTER-API-KEY-HERE
LLM_TEMPERATURE=0.0
LLM_MAX_COMPLETION_TOKENS=16384

# OpenAI compatibility settings
OPENAI_API_BASE=https://openrouter.ai/api/v1
OPENAI_API_KEY=YOUR-OPENROUTER-API-KEY-HERE

# ============================================================================
# Embedding Configuration
# ============================================================================
# Using FastEmbed (local embeddings) - no API calls needed
# This avoids tokenizer issues and provides fast, free embeddings
EMBEDDING_PROVIDER=fastembed
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSIONS=384
EMBEDDING_MAX_TOKENS=256

# ============================================================================
# Vector Database Configuration (LanceDB)
# ============================================================================
# Cognee 0.3.6 uses LanceDB by default (no Qdrant needed)
# Data will be stored in: .venv/lib/python3.12/site-packages/cognee/.cognee_system/databases

# Optional: If you want to use Qdrant instead
# QDRANT_URL=:memory:
# QDRANT_API_KEY=

# ============================================================================
# Graph Database Configuration
# ============================================================================
# Cognee 0.3.6 requires a real graph database (NetworkX no longer supported)
# Using Kuzu - embedded graph database (no separate server needed)
GRAPH_DATABASE_PROVIDER=kuzu

# Optional: For Neo4j (requires separate server)
# GRAPH_DATABASE_PROVIDER=neo4j
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=password

# ============================================================================
# Tokenizer Configuration
# ============================================================================
# Suppress tokenizer parallelism warnings
TOKENIZERS_PARALLELISM=false

# ============================================================================
# Storage Configuration
# ============================================================================
# Cognee uses default local storage in .cognee_system directory
# No additional configuration needed for local development
# Data is stored in: .venv/lib/python3.12/site-packages/cognee/.cognee_system/

# ============================================================================
# Authentication (Optional)
# ============================================================================
# Set to "true" to enable authentication
# REQUIRE_AUTHENTICATION=false
# DEFAULT_USER_EMAIL=admin@example.com
# DEFAULT_USER_PASSWORD=admin123

# ============================================================================
# Monitoring (Optional - Langfuse)
# ============================================================================
# LANGFUSE_HOST=https://cloud.langfuse.com
# LANGFUSE_PUBLIC_KEY=your-public-key
# LANGFUSE_SECRET_KEY=your-secret-key

# ============================================================================
# Rate Limiting (Optional)
# ============================================================================
# LLM_RATE_LIMIT_ENABLED=false
# LLM_RATE_LIMIT_REQUESTS=60
# LLM_RATE_LIMIT_INTERVAL=60
# EMBEDDING_RATE_LIMIT_ENABLED=false
# EMBEDDING_RATE_LIMIT_REQUESTS=60
# EMBEDDING_RATE_LIMIT_INTERVAL=60

# ============================================================================
# AWS Configuration (for Multi-Modal Document Loading)
# ============================================================================
# AWS credentials (use AWS CLI configuration or environment variables)
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1

# S3 bucket containing processed documents (Textract JSON + page images)
S3_PROCESSED_BUCKET=your-processed-bucket-name

# DynamoDB table for document tracking
DDB_DOCUMENTS_TABLE=your-documents-table-name
